import gc
import json
import math
import random
import string
import time
from copy import deepcopy
from typing import Optional, Any
import re
import os
import ast

import dill
import numpy as np
import pandas as pd
import torch
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from fastchat.model import get_conversation_template
from transformers import (AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel,
                          GPTJForCausalLM, GPTNeoXForCausalLM,
                          LlamaForCausalLM, GemmaForCausalLM, Gemma2ForCausalLM)

# FLAG
SIMULATED_CANONICAL = True


class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


def get_embedding_layer(model):
    if isinstance(model, GPTJForCausalLM) or isinstance(model, GPT2LMHeadModel):
        return model.transformer.wte
    elif isinstance(model, LlamaForCausalLM):
        return model.model.embed_tokens
    elif isinstance(model, GPTNeoXForCausalLM):
        return model.base_model.embed_in
    elif isinstance(model, GemmaForCausalLM):
        return model.model.embed_tokens
    elif isinstance(model, Gemma2ForCausalLM):
        return model.model.embed_tokens
    else:
        raise ValueError(f"Unknown model type: {type(model)}")


def get_embedding_matrix(model):
    if isinstance(model, GPTJForCausalLM) or isinstance(model, GPT2LMHeadModel):
        return model.transformer.wte.weight
    elif isinstance(model, LlamaForCausalLM):
        return model.model.embed_tokens.weight
    elif isinstance(model, GemmaForCausalLM):
        return model.model.embed_tokens.weight
    elif isinstance(model, Gemma2ForCausalLM):
        return model.model.embed_tokens.weight
    elif isinstance(model, GPTNeoXForCausalLM):
        return model.base_model.embed_in.weight
    else:
        raise ValueError(f"Unknown model type: {type(model)}")


def get_embeddings(model, input_ids):
    if isinstance(model, GPTJForCausalLM) or isinstance(model, GPT2LMHeadModel):
        return model.transformer.wte(input_ids).half()
    elif isinstance(model, LlamaForCausalLM):
        return model.model.embed_tokens(input_ids)
    elif isinstance(model, GemmaForCausalLM):
        return model.model.embed_tokens(input_ids)
    elif isinstance(model, Gemma2ForCausalLM):
        return model.model.embed_tokens(input_ids)
    elif isinstance(model, GPTNeoXForCausalLM):
        return model.base_model.embed_in(input_ids).half()
    else:
        raise ValueError(f"Unknown model type: {type(model)}")


def get_nonascii_toks(tokenizer, device='cpu', aggressive=False):
    def is_ascii(s):
        return s.isascii() and s.isprintable()

    def is_punctuation(s):
        return s in string.punctuation and not (s in ".,?!; ")

    def is_alphanumeric_or_special_chars(s):
        alphanumeric_found = False
        special_chars_found = False

        if s in [".", ",", " "]:
            return True

        for char in s:
            if char.isalpha():
                continue
                if special_chars_found:
                    return False
                alphanumeric_found = True
            #elif char in ".,_ ":
            elif char in " ":
                continue
                if alphanumeric_found:
                    return False
                special_chars_found = True
            else:
                return False

        return True

    def check_inflation(tokenizer, k):
        # to avoid the incident of choosing a token that inflates the length of the tokenized string

        tmpstr = tokenizer.decode([k])
        if tokenizer.vocab_size == 32000:  # llama2
            starting_point = 0
        else:
            starting_point = 1

        if tokenizer(tmpstr).input_ids[starting_point:] != [k]:
            return True
        return False

    ascii_toks = []
    for i in range(3, tokenizer.vocab_size):
        # if not is_ascii(tokenizer.decode([i])) or is_punctuation(tokenizer.decode([i])):
        if not is_ascii(tokenizer.decode([i])) or not is_alphanumeric_or_special_chars(tokenizer.decode([i])):
            ascii_toks.append(i)
        elif check_inflation(tokenizer, i) and aggressive:
            ascii_toks.append(i)

    if tokenizer.bos_token_id is not None:
        ascii_toks.append(tokenizer.bos_token_id)
    if tokenizer.eos_token_id is not None:
        ascii_toks.append(tokenizer.eos_token_id)
    if tokenizer.pad_token_id is not None:
        ascii_toks.append(tokenizer.pad_token_id)
    if tokenizer.unk_token_id is not None:
        ascii_toks.append(tokenizer.unk_token_id)

    return torch.tensor(ascii_toks, device=device)


class Prompter(object):
    """
    A class used to generate an attack prompt.
    """

    def __init__(self,
                 goal,
                 target,
                 tokenizer,
                 conv_template,
                 control_init="! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !",
                 test_prefixes=["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"],
                 final_target=[],
                 *args, **kwargs
                 ):
        """
        Initializes the AttackPrompt object with the provided parameters.

        Parameters
        ----------
        goal : str
            The intended goal of the attack
        target : str
            The target of the attack
        tokenizer : Transformer Tokenizer
            The tokenizer used to convert text into tokens
        conv_template : Template
            The conversation template used for the attack
        control_init : str, optional
            A string used to control the attack (default is "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ")
        test_prefixes : list, optional
            A list of prefixes to test the attack (default is ["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"])
        """

        self.goal = goal
        self.target = target
        self.control = control_init
        self.tokenizer = tokenizer
        self.control_pos = "post"
        self.conv_template = conv_template
        self.test_prefixes = test_prefixes
        self.final_target = final_target
        self.current_solution = "So "
        self.control_len = 0  # for the sake of initialization

        self.conv_template.messages = []
        self.test_new_toks = len(self.tokenizer(self.target).input_ids) + 2  # buffer
        for prefix in self.test_prefixes:
            self.test_new_toks = max(self.test_new_toks, len(self.tokenizer(prefix).input_ids))

        self._update_ids()

    def _update_ids(self):

        def find_last_subarray_indices(tokenizer, array1, str2):
            array2 = tokenizer(str2).input_ids
            if 'Llama-3' in tokenizer.name_or_path:
                array2 = array2[1:]  # because it never stops generating the first starting token
            len_array2 = len(array2)
            for i in range(len(array1) - len_array2, len(array1) - len_array2 -10, -1):
                if array1[i:i + len_array2] == array2:
                    return i, i + len_array2

            # Since we did not get any return value, it indicates tokenizer issue with leading space. So, we try again with a leading space.
            array2 = tokenizer((" " +str2)).input_ids
            if 'Llama-3' in tokenizer.name_or_path:
                array2 = array2[1:]
            len_array2 = len(array2)
            for i in range(len(array1) - len_array2, -1, -1):
                if array1[i:i + len_array2] == array2:
                    return i, i + len_array2

            return -1, -1  # Return -1, -1 if array2 is not found in array1

        if self.control_pos == "post":
            self.conv_template.append_message(self.conv_template.roles[0], f"\"{self.goal}\" {self.control}")
        else:
            self.conv_template.append_message(self.conv_template.roles[0], f"\"{self.control}\" {self.goal}")

        self.conv_template.append_message(self.conv_template.roles[1], f"{self.target}")

        prompt = self.conv_template.get_prompt()
        # prompt = re.sub(start_delim + ".*?" + end_delim, replacement, prompt, flags=re.DOTALL)
        encoding = self.tokenizer(prompt)
        toks = encoding.input_ids

        if self.conv_template.name == 'llama-2':
            self.conv_template.messages = []

            self.conv_template.append_message(self.conv_template.roles[0], "")
            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids

            self._user_role_slice = slice(None, len(toks) + 1)  # FORCED BUG FIX for accurate slicing.

            if self.control_pos == "post":
                self.conv_template.update_last_message(f"{self.goal}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)))

                separator = ' ' if self.goal else ''
                self.conv_template.update_last_message(f"{self.goal}{separator}{self.control}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._control_slice = slice(self._goal_slice.stop, len(toks))

                self.conv_template.update_last_message(f"{self.goal}{separator}{self.control}{separator}")

            else:
                self.conv_template.update_last_message(f"{self.control}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._control_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)))

                separator = " " if self.goal else ''
                self.conv_template.update_last_message(f"{self.control}{separator}{self.goal}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._goal_slice = slice(self._control_slice.stop, len(toks))

                self.conv_template.update_last_message(f"{self.control}{separator}{self.goal}{separator}")

            self.conv_template.append_message(self.conv_template.roles[1], None)
            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
            if self.control_pos == "post":
                self._assistant_role_slice = slice(self._control_slice.stop, len(toks))
            else:
                self._assistant_role_slice = slice(self._goal_slice.stop, len(toks))

            # TODO here we are assuming that target is not "CANONICAL". This must be handled for GSM8K where target is canonical
            if SIMULATED_CANONICAL:
                self.conv_template.update_last_message(f"{self.current_solution}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._current_solution_slice = slice(self._assistant_role_slice.stop, len(toks)-2)

                self.conv_template.update_last_message(f"{self.current_solution} {self.target}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._target_slice = slice(self._current_solution_slice.stop, len(toks)-2)
                self._loss_slice = slice(self._current_solution_slice.stop-1, len(toks)-3)
            else:
                self.conv_template.update_last_message(f"{self.target}")
                toks = self.tokenizer(self.conv_template.get_prompt()).input_ids
                self._target_slice = slice(self._assistant_role_slice.stop, len(toks) - 2)
                self._loss_slice = slice(self._assistant_role_slice.stop - 1, len(toks) - 3)

            if len(self.final_target) > 0:  # focused answer exists
                # bias1, bias2 = find_last_substring_indices(self.target, self.final_target)
                idx1, idx2 = find_last_subarray_indices(self.tokenizer, toks, self.final_target)
                self._focused_target_slice = slice(idx1, idx2)
            else:
                self._focused_target_slice = None
                # self._focused_target_slice = 0

        elif self.conv_template.name == 'llama-3':
            self.conv_template.messages = []
            full_input = ""

            # user role slice
            full_input += "<|start_header_id|>user<|end_header_id|>\n\n"  # are u sure?
            toks = self.tokenizer(full_input).input_ids
            self._user_role_slice = slice(None, len(toks))

            if self.control_pos == "post":
                # goal_slice and control_slice and assistant role slice
                # goal_slice
                separator = " "
                full_input += self.goal
                # full_input += " "
                toks = self.tokenizer(full_input).input_ids
                self._goal_slice = slice(self._user_role_slice.stop, len(toks))

                # control slice
                if self.control.startswith(" "):
                    self.control = self.control[1:]
                full_input = full_input + " " + self.control
                toks = self.tokenizer(full_input).input_ids

                if self.control_len == 0:
                    self.control_len = len(toks)

                self._control_slice = slice(self._goal_slice.stop, len(toks))
            elif self.control_pos == "pre":
                # control_slice and goal_slice and assistant role slice
                # control slice
                full_input += self.control
                toks = self.tokenizer(full_input).input_ids
                self._control_slice = slice(self._user_role_slice.stop, len(toks))

                # goal_slice
                full_input += " "
                full_input += self.goal
                toks = self.tokenizer(full_input).input_ids
                self._goal_slice = slice(self._control_slice.stop, len(toks))

            # assistant role slice
            full_input += "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            toks = self.tokenizer(full_input).input_ids
            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))

            # current solution slice
            if SIMULATED_CANONICAL:
                full_input += self.current_solution
                toks = self.tokenizer(full_input).input_ids
                self._current_solution_slice = slice(self._assistant_role_slice.stop, len(toks))

                # target_slice
                full_input += " " ## added on Sept 15, 2024
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._current_solution_slice.stop, len(toks))
                self._loss_slice = slice(self._current_solution_slice.stop - 1, len(toks) - 1)
            else:
                # target_slice
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._assistant_role_slice.stop, len(toks))
                self._loss_slice = slice(self._assistant_role_slice.stop - 1, len(toks) - 1)

            if len(self.final_target) > 0:  # focused answer exists
                idx1, idx2 = find_last_subarray_indices(self.tokenizer, toks, self.final_target)
                self._focused_target_slice = slice(idx1, idx2)
            else:
                self._focused_target_slice = None

        elif self.conv_template.name == 'gemma-2':
            self.conv_template.messages = []
            full_input = ""

            # user role slice
            full_input += "<bos><start_of_turn>user\n"
            toks = self.tokenizer(full_input).input_ids
            self._user_role_slice = slice(None, len(toks))

            if self.control_pos == "post":
                separator = " "
                # goal_slice
                full_input += self.goal
                toks = self.tokenizer(full_input).input_ids
                self._goal_slice = slice(self._user_role_slice.stop, len(toks))

                # control slice
                if self.control.startswith(" "):
                    self.control = self.control[1:]
                full_input = full_input + " " + self.control
                toks = self.tokenizer(full_input).input_ids
                self._control_slice = slice(self._goal_slice.stop, len(toks))
            elif self.control_pos == "pre":
                raise NotImplementedError # Not necessary to be implemented in our protocol

            # assistant role slice
            full_input += "<end_of_turn>\n<start_of_turn>model\n"
            toks = self.tokenizer(full_input).input_ids
            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))

            # current solution slice
            if SIMULATED_CANONICAL:
                full_input += self.current_solution
                toks = self.tokenizer(full_input).input_ids
                self._current_solution_slice = slice(self._assistant_role_slice.stop, len(toks))

                # target_slice
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._current_solution_slice.stop, len(toks))
                self._loss_slice = slice(self._current_solution_slice.stop - 1, len(toks) - 1)
            else:
                # target_slice
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._assistant_role_slice.stop, len(toks))
                self._loss_slice = slice(self._assistant_role_slice.stop - 1, len(toks) - 1)

            if len(self.final_target) > 0:  # focused answer exists
                idx1, idx2 = find_last_subarray_indices(self.tokenizer, toks, self.final_target)
                self._focused_target_slice = slice(idx1, idx2)
            else:
                self._focused_target_slice = None


        elif self.conv_template.name == 'gemma':
            # Handle everything manually from absolute scratch since fschat doesnot give full support

            # TODO introduce prefix support as well

            self.conv_template.messages = []
            full_input = ""
            # user role slice
            full_input += "<bos>"
            toks = self.tokenizer(full_input).input_ids
            self._user_role_slice = slice(None, len(toks))

            # goal slice
            full_input += self.goal
            toks = self.tokenizer(full_input).input_ids
            self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)))

            # control slice
            separator = " "
            full_input = full_input + separator + self.control
            toks = self.tokenizer(full_input).input_ids
            self._control_slice = slice(self._goal_slice.stop, len(toks))

            # assistant role slice
            full_input += "\n\n"
            toks = self.tokenizer(full_input).input_ids
            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))

            # current solution slice
            if SIMULATED_CANONICAL:
                full_input += self.current_solution
                toks = self.tokenizer(full_input).input_ids
                self._current_solution_slice = slice(self._assistant_role_slice.stop, len(toks))

                # target_slice
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._current_solution_slice.stop, len(toks))
                self._loss_slice = slice(self._current_solution_slice.stop - 1, len(toks) - 1)

            # target slice
            else:
                full_input += self.target
                toks = self.tokenizer(full_input).input_ids
                self._target_slice = slice(self._assistant_role_slice.stop, len(toks))
                self._loss_slice = slice(self._assistant_role_slice.stop - 1, len(toks) - 1)

            if len(self.final_target) > 0:  # focused answer exists
                # bias1, bias2 = find_last_substring_indices(self.target, self.final_target)
                idx1, idx2 = find_last_subarray_indices(self.tokenizer, toks, self.final_target)
                self._focused_target_slice = slice(idx1, idx2)

            else:
                self._focused_target_slice = None
                # self._focused_target_slice = 0

        self.input_ids = torch.tensor(toks[:self._target_slice.stop], device='cpu')
        self.conv_template.messages = []

    @torch.no_grad()
    def generate(self, model, gen_config=None):
        if gen_config is None:
            gen_config = model.generation_config
            gen_config.max_new_tokens = 256 # unused

        if gen_config.max_new_tokens > 128: # unused
            print('WARNING: max_new_tokens > 32 may cause testing to slow down.')
        input_ids = self.input_ids[:self._assistant_role_slice.stop].to(model.device).unsqueeze(0)
        attn_masks = torch.ones_like(input_ids).to(model.device)
        output_ids = model.generate(input_ids,
                                    attention_mask=attn_masks,
                                    generation_config=gen_config,
                                    max_new_tokens=1024,
                                    pad_token_id=self.tokenizer.pad_token_id,
                                    do_sample=False)[0]
        return output_ids[self._assistant_role_slice.stop:]

    def generate_str(self, model, gen_config=None):
        return self.tokenizer.decode(self.generate(model, gen_config), skip_special_tokens=True)

    def update_solution(self, model, gen_config=None):
        if gen_config is None:
            gen_config = model.generation_config
            gen_config.max_new_tokens = 512
        gen_config.skip_special_tokens = True
        gen_str = self.generate_str(model, gen_config).strip()
        self.current_solution = gen_str

        self._update_ids()


    def test(self, model, gen_config=None):
        # useless

        if gen_config is None:
            gen_config = model.generation_config
            gen_config.max_new_tokens = self.test_new_toks
        gen_str = self.generate_str(model, gen_config).strip()
        print(gen_str)
        jailbroken = not any([prefix in gen_str for prefix in self.test_prefixes])
        em = self.target in gen_str
        return jailbroken, int(em)

    @torch.no_grad()
    def test_loss(self, model):
        logits, ids = self.logits(model, return_ids=True)
        return self.target_loss(logits, ids).mean().item()

    def grad(self, model, current_pos, valid_tokens):

        raise NotImplementedError("Gradient function not yet implemented")

    @torch.no_grad()
    def logits(self, model, test_controls=None, return_ids=False):
        pad_tok = -1
        if test_controls is None:
            test_controls = self.control_toks
        if isinstance(test_controls, torch.Tensor):
            if len(test_controls.shape) == 1:
                test_controls = test_controls.unsqueeze(0)
            test_ids = test_controls.to(model.device)
        elif not isinstance(test_controls, list):
            test_controls = [test_controls]
        elif isinstance(test_controls[0], str):
            max_len = self._control_slice.stop - self._control_slice.start
            test_ids = [
                torch.tensor(self.tokenizer(control, add_special_tokens=False).input_ids[:max_len], device=model.device)
                for control in test_controls
            ]
            pad_tok = 0
            while pad_tok in self.input_ids or any([pad_tok in ids for ids in test_ids]):
                pad_tok += 1
            nested_ids = torch.nested.nested_tensor(test_ids)
            test_ids = torch.nested.to_padded_tensor(nested_ids, pad_tok, (len(test_ids), max_len))
        else:
            raise ValueError(
                f"test_controls must be a list of strings or a tensor of token ids, got {type(test_controls)}")

        if not (test_ids[0].shape[0] == self._control_slice.stop - self._control_slice.start):
            raise ValueError((
                f"test_controls must have shape "
                f"(n, {self._control_slice.stop - self._control_slice.start}), "
                f"got {test_ids.shape}"
            ))

        locs = torch.arange(self._control_slice.start, self._control_slice.stop).repeat(test_ids.shape[0], 1).to(
            model.device)
        ids = torch.scatter(
            self.input_ids.unsqueeze(0).repeat(test_ids.shape[0], 1).to(model.device),
            1,
            locs,
            test_ids
        )
        if pad_tok >= 0:
            attn_mask = (ids != pad_tok).type(ids.dtype)
        else:
            attn_mask = None

        if return_ids:
            del locs, test_ids;
            gc.collect()
            return model(input_ids=ids, attention_mask=attn_mask).logits, ids
        else:
            del locs, test_ids
            logits = model(input_ids=ids, attention_mask=attn_mask).logits
            del ids;
            gc.collect()
            return logits

    def selection_loss(self, logits, target_slice_start ,target_ids, control_weight=0.2):

        temperature = 0.2
        target_ids = target_ids.to(logits.device)
        shiftamt = target_slice_start - self._target_slice.start
        crit = nn.CrossEntropyLoss(reduction='none')

        loss_slice = slice(self._target_slice.start - 1 + shiftamt, self._target_slice.stop - 1 + shiftamt)
        loss = crit(logits[loss_slice, :], target_ids)
        focused_target_slice_start = self._focused_target_slice.start +shiftamt
        focused_target_slice_end = self._focused_target_slice.stop +shiftamt
        window_size = self._focused_target_slice.stop - self._focused_target_slice.start

        control_loss_slice = slice(self._control_slice.start - 1, self._control_slice.stop - 1)
        control_temp_slice = slice(self._control_slice.start, self._control_slice.stop)
        logsftmaxed = F.log_softmax(logits[control_loss_slice, :], dim=-1)
        t_logsftmaxed = logsftmaxed[torch.arange(logits[control_loss_slice, :].shape[0]), self.input_ids[control_temp_slice]] ###########################################

        if self._focused_target_slice:
            focused_loss_slice = slice(focused_target_slice_start - 1, focused_target_slice_end - 1)
            logits = logits/temperature
            focused_loss = nn.CrossEntropyLoss(reduction='none')(logits[focused_loss_slice, :], target_ids[self._focused_target_slice.start-self._target_slice.start: self._focused_target_slice.start-self._target_slice.start+window_size])
            focused_mist_rate = (logits[focused_loss_slice, :].argmax(dim=-1)!=target_ids[self._focused_target_slice.start-self._target_slice.start: self._focused_target_slice.start-self._target_slice.start+window_size]).any().float()


            del logits; gc.collect()
            return focused_loss.unsqueeze(-1), focused_mist_rate.unsqueeze(-1), (-t_logsftmaxed).unsqueeze(-1)
            # loss = 0.0 * loss + 1 * focused_loss
            # return loss.unsqueeze(-1)
        else:
            return loss

    def target_loss(self, logits, ids):
        crit = nn.CrossEntropyLoss(reduction='none')
        loss_slice = slice(self._target_slice.start - 1, self._target_slice.stop - 1)
        loss = crit(logits[:, loss_slice, :].transpose(1, 2), ids[:, self._target_slice])
        # return loss
        if self._focused_target_slice:
            MAX_CONSIDERED = 5
            loss_logits = logits[:, self._loss_slice, :]
            window_size = self._focused_target_slice.stop - self._focused_target_slice.start
            unfolded = loss_logits.unfold(1, window_size, 1).transpose(2, 3)
            unfolded = unfolded[:, -MAX_CONSIDERED:, :, :]  # considering last MAX_CONSIDERED WINDOWS
            focused_target = ids[:, self._focused_target_slice].unsqueeze(1).repeat(1, unfolded.shape[1], 1)
            # focused_target = ids[self._focused_target_slice].repeat(len(unfolded), 1)
            flat_unfolded = unfolded.reshape(-1, unfolded.shape[-1])
            focused_target = focused_target.flatten()
            focused_loss = nn.CrossEntropyLoss(reduction='none')(flat_unfolded, focused_target)
            focused_loss = focused_loss.view(unfolded.shape[0], -1, window_size).mean(dim=2)
            focused_loss = focused_loss.min(dim=1).values

            return focused_loss.unsqueeze(-1)
        return loss

    def control_loss(self, logits, ids):

        loss_slice = slice(self._control_slice.start - 1, self._control_slice.stop - 1)
        temp_slice = slice(self._control_slice.start, self._control_slice.stop)

        logsftmaxed = F.log_softmax(logits[:, loss_slice, :], dim=2)
        stacked_perplexities = torch.stack(
            [logsftmaxed[i, torch.arange(logits[:, loss_slice, :].shape[1]), ids[i, temp_slice]] for i in
             range(0, len(logsftmaxed))])
        # tt = torch.gather(logsftmaxed, 1, ids[:, temp_slice].unsqueeze(2)).squeeze(2)

        return -stacked_perplexities

    @property
    def assistant_str(self):
        return self.tokenizer.decode(self.input_ids[self._assistant_role_slice]).strip()

    @property
    def assistant_toks(self):
        return self.input_ids[self._assistant_role_slice]

    @property
    def goal_str(self):
        return self.tokenizer.decode(self.input_ids[self._goal_slice]).strip()

    @goal_str.setter
    def goal_str(self, goal):
        self.goal = goal
        self._update_ids()

    @property
    def goal_toks(self):
        return self.input_ids[self._goal_slice]

    @property
    def target_str(self):
        return self.tokenizer.decode(self.input_ids[self._target_slice]).strip()

    @target_str.setter
    def target_str(self, target):
        self.target = target
        self._update_ids()

    @property
    def target_toks(self):
        return self.input_ids[self._target_slice]

    @property
    def control_str(self):
        return self.tokenizer.decode(self.input_ids[self._control_slice]).strip()

    @control_str.setter
    def control_str(self, control):
        self.control = control
        self._update_ids()

    @property
    def control_toks(self):
        return self.input_ids[self._control_slice]

    @control_toks.setter
    def control_toks(self, control_toks):
        self.control = self.tokenizer.decode(control_toks)
        self._update_ids()

    @property
    def current_solution_str(self):
        return self.tokenizer.decode(self.input_ids[self._current_solution_slice]).strip()

    @current_solution_str.setter
    def current_solution_str(self, current_solution):
        self.current_solution = current_solution
        self._update_ids()

    @property
    def current_solution_toks(self):
        return self.input_ids[self._current_solution_slice]

    @current_solution_toks.setter
    def current_solution_toks(self, current_solution_toks):
        self.current_solution = self.tokenizer.decode(current_solution_toks)
        self._update_ids()

    @property
    def prompt(self):
        return self.tokenizer.decode(self.input_ids[self._goal_slice.start:self._control_slice.stop])

    @property
    def input_toks(self):
        return self.input_ids

    @property
    def input_str(self):
        return self.tokenizer.decode(self.input_ids)

    @property
    def eval_str(self):
        return self.tokenizer.decode(self.input_ids[:self._assistant_role_slice.stop]).replace('<s>', '').replace(
            '</s>', '')


class PromptManager(object):
    """A class used to manage the prompt during optimization."""

    def __init__(self,
                 goals,
                 targets,
                 tokenizer,
                 conv_template,
                 control_init="! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !",
                 test_prefixes=["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"],
                 managers=None,
                 final_targets=[],
                 *args, **kwargs
                 ):
        """
        Initializes the PromptManager object with the provided parameters.

        Parameters
        ----------
        goals : list of str
            The list of intended goals of the attack
        targets : list of str
            The list of targets of the attack
        tokenizer : Transformer Tokenizer
            The tokenizer used to convert text into tokens
        conv_template : Template
            The conversation template used for the attack
        control_init : str, optional
            A string used to control the attack (default is "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !")
        test_prefixes : list, optional
            A list of prefixes to test the attack (default is ["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"])
        managers : dict, optional
            A dictionary of manager objects, required to create the prompts.
        """

        if len(goals) != len(targets):
            raise ValueError("Length of goals and targets must match")
        if len(goals) == 0:
            raise ValueError("Must provide at least one goal, target pair")

        self.tokenizer = tokenizer

        self._prompts = [
            managers['AP'](
                goal,
                target,
                tokenizer,
                conv_template,
                control_init,
                test_prefixes,
                final_target
            )
            for goal, target, final_target in zip(goals, targets, final_targets)
        ]

        self._nonascii_toks = get_nonascii_toks(tokenizer, device='cpu', aggressive=False)

    def generate(self, model, gen_config=None):
        if gen_config is None:
            gen_config = model.generation_config
            gen_config.max_new_tokens = 128

        return [prompt.generate(model, gen_config) for prompt in self._prompts]

    @torch.no_grad()
    def generate_batched(self, model, prompts, prompt_candidate_toks =None, gen_config=None, return_past_key_vals=False):

        if not prompt_candidate_toks:
            prompt_candidate_toks = prompts[0].input_ids[prompts[0]._control_slice]

        if gen_config is None:
            gen_config = model.generation_config
            gen_config.max_length = 700

        # Extract and slice input_ids from each Prompt object
        sliced_input_ids_list = []
        for prompt in prompts:
            temp = torch.tensor(prompt.input_ids[:prompt._assistant_role_slice.stop])
            temp[prompt._control_slice] = prompt_candidate_toks
            sliced_input_ids_list.append(temp)

        # Find the length of the longest sequence to calculate padding
        max_len = max([len(seq) for seq in sliced_input_ids_list])


        input_ids_padded = []

        for seq in sliced_input_ids_list:
            padded_seq = torch.cat([torch.full((max_len - len(seq),), self.tokenizer.pad_token_id), seq])
            input_ids_padded.append(padded_seq)

        input_ids_padded = torch.stack(input_ids_padded).to(model.device)

        # Create attention masks (1 for non-padding tokens, 0 for padding tokens)
        attn_masks = (input_ids_padded != self.tokenizer.pad_token_id).to(model.device)

        # Pad input_ids to the length of the longest sequence
        # input_ids_padded = pad_sequence(sliced_input_ids_list,
        #                                 batch_first=True,
        #                                 padding_value=self.tokenizer.pad_token_id).to(model.device)

        # Create attention masks
        #attn_masks = (input_ids_padded != self.tokenizer.pad_token_id).to(model.device)

        # # Find the minimum length among all sequences
        # min_length = min([len(seq) for seq in sliced_input_ids_list])
        #
        # ##### <> #####
        #
        # # Truncate all sequences to the minimum length
        # sliced_input_ids_list = [seq[:min_length] for seq in sliced_input_ids_list]
        #
        # # Stack them into a tensor (no need to pad since they are all the same length now)
        # input_ids_padded = torch.stack(sliced_input_ids_list).to(model.device)
        #
        # # Create attention masks (all ones since no padding is used)
        # attn_masks = torch.ones(input_ids_truncated.size(), dtype=torch.long).to(model.device)

        # Perform generation
        model.eval()
        with torch.no_grad():
            output_ids = model.generate(input_ids_padded,
                                    attention_mask=attn_masks,
                                    generation_config=gen_config,
                                    max_new_tokens=1024,
                                    output_hidden_states=False, output_attentions=False, output_logits=False,
                                    pad_token_id=self.tokenizer.pad_token_id,
                                    do_sample=False, return_dict_in_generate=return_past_key_vals)

        if return_past_key_vals:
            output_ids, past_key_vals = output_ids.sequences, output_ids.past_key_values
            print("Returned with past_key_vals. Warning: This can be slower")

        # Extract the generated tokens, excluding the original input length
        result = []

        # TODO possibility of faster implementation later
        for i, ids in enumerate(input_ids_padded):
            result.append(output_ids[i, len(ids):])

        if not return_past_key_vals:
            return result

        return result, output_ids, past_key_vals # return past_key_vals and output_ids if requested


    def generate_str(self, model, gen_config=None):
        return [
            self.tokenizer.decode(output_toks)
            for output_toks in self.generate(model, gen_config)
        ]

    def generate_batched_str(self, model, prompts, prompt_candidate_toks =None, gen_config=None):
        # batch generation often causes the assistant token to be repeated, so manually filter them out
        assistant_str = self.tokenizer.decode(self._prompts[0].input_ids[self._prompts[0]._assistant_role_slice], skip_special_tokens = True) # TODO: assumes all prompts have the same assistant role slice

        # TODO can be faster
        return [
            self.tokenizer.decode(output_toks, skip_special_tokens=True).split(assistant_str)[-1].strip()
            for output_toks in self.generate_batched(model, prompts, prompt_candidate_toks, gen_config)
        ]

    def update_solution(self, model, gen_config=None, generation_batch_size=9):

        stpwatch_strt = time.time()
        for i in range(0, len(self._prompts), generation_batch_size):
            batch = self._prompts[i:i + generation_batch_size]
            outputs = self.generate_batched_str(model, batch, gen_config)
            for prompt, output in zip(batch, outputs):
                prompt.current_solution_str = output
        print("Time taken to update solutions: ", time.time() - stpwatch_strt)
                #
        # [prompt.update_solution(model, gen_config) for prompt in self._prompts]

    def test(self, model, gen_config=None):
        return [prompt.test(model, gen_config) for prompt in self._prompts]

    def test_loss(self, model):
        return [prompt.test_loss(model) for prompt in self._prompts]

    def grad(self, model, current_pos, valid_tokens):
        return sum([prompt.grad(model, current_pos, valid_tokens) for prompt in self._prompts])

    def update_slices(self, input_ids, control_slice,
                      current_solution_slice, target_slice,
                      focused_target_slice, new_current_solution, new_control):

        # Extract indices from slices
        control_start, control_end = control_slice.start, control_slice.stop
        current_solution_start, current_solution_end = current_solution_slice.start, current_solution_slice.stop
        target_start, target_end = target_slice.start, target_slice.stop
        focused_target_start, focused_target_end = focused_target_slice.start, focused_target_slice.stop

        # Convert the new solutions to tensors if they're not already
        new_current_solution = torch.tensor(new_current_solution)
        new_control = torch.tensor(new_control)

        # Ensure new_control is the same length as the original control slice
        if new_control.size(0) != (control_end - control_start):
            raise ValueError("new_control must be the same length as the original control slice")

        # Calculate the new end index for the current solution slice
        new_current_solution_end = current_solution_start + new_current_solution.size(0)

        # Update input_ids tensor
        updated_input_ids = torch.cat((
            input_ids[:control_start],
            new_control,
            input_ids[control_end:current_solution_start],
            new_current_solution,
            input_ids[current_solution_end:]
        ), dim=0)

        # Update slice indices
        shift_amount = new_current_solution.size(0) - (current_solution_end - current_solution_start)

        new_control_slice = slice(control_start, control_end)
        new_current_solution_slice = slice(current_solution_start + shift_amount,
                                           new_current_solution_end + shift_amount)
        new_target_slice = slice(target_start + shift_amount, target_end + shift_amount)
        new_focused_target_slice = slice(focused_target_start + shift_amount, focused_target_end + shift_amount)

        return updated_input_ids, new_control_slice, new_current_solution_slice, new_target_slice, new_focused_target_slice


    def logits(self, model, test_controls=None, return_ids=False):
        vals = [prompt.logits(model, test_controls, return_ids) for prompt in self._prompts]
        if return_ids:
            return [val[0] for val in vals], [val[1] for val in vals]
        else:
            return vals

    def pad_with_last_value(self, tensor, max_dim1):
        pad_size = max_dim1 - tensor.size(1)
        if pad_size > 0:
            last_values = tensor[:, -1:, :]  # Take the last slice in the second dimension
            padding = last_values.repeat(1, pad_size, 1)  # Repeat the last slice
            tensor = torch.cat([tensor, padding], dim=1)  # Concatenate along the second dimension
        return tensor

    @torch.no_grad()
    def logits_batched_gen(self, model, prompt_candidates_toks, logits_batch_size=9, percentage=0.5, iter_index=0):
        # percentage: percentage of the target tokens to be focused on
        model.eval()
        stpwatch_start = time.time()

        # listing all the prompt with target prompt tokens
        max_new_toks = 1024
        sliced_input_ids_list = []
        targets_list = []
        focused_targets_list = []
        #focus_offset_list = []
        logits_all = []
        target_starts = []
        padding_counts_list = []
        _prompts_obj_list = []
        random.seed(random.randint(0, 99999))

        length = len(self._prompts)

        if length < 30:
            percentage = 1.0 # avoid droppping samples in case of small number of samples

        for ind, prompt in enumerate(self._prompts):
            if ind >= length * percentage * iter_index and ind < length * percentage * (iter_index+1):
                _prompts_obj_list.append(prompt)

        #for prompt in self._prompts:
        for prompt in _prompts_obj_list:

            temp = torch.tensor(prompt.input_ids[:prompt._assistant_role_slice.stop])
            temp[prompt._control_slice] = prompt_candidates_toks
            sliced_input_ids_list.append(temp)
            targets_list.append(prompt.input_ids[prompt._target_slice])
            focused_targets_list.append(prompt.input_ids[prompt._focused_target_slice])

        for i in range(0, len(_prompts_obj_list), logits_batch_size):
            batched_ids = sliced_input_ids_list[i:i + logits_batch_size]
            batched_target = targets_list[i:i + logits_batch_size]

            batched_ids_reversed = [torch.flip(item, dims=[0]) for item in batched_ids]
            batched_ids_padded = pad_sequence(batched_ids_reversed, batch_first=True,
                                              padding_value=self.tokenizer.pad_token_id).to(model.device)
            batched_ids_padded = torch.flip(batched_ids_padded, dims=[1])  # Flip them back to original order

            # Update attention masks accordingly
            attention_mask_ids_padded = (batched_ids_padded != self.tokenizer.pad_token_id).to(model.device)

            with torch.no_grad():
                # output = model.generate(input_ids=batched_ids_padded, attention_mask=attention_mask_ids_padded,
                #                         max_length=max_new_toks, return_dict_in_generate=False, output_scores=False,
                #                         output_hidden_states=False, output_attentions=False, output_logits=False, do_sample=False)

                output = model.generate(input_ids=batched_ids_padded, attention_mask=attention_mask_ids_padded,return_dict_in_generate=False, output_scores=False,pad_token_id=self.tokenizer.pad_token_id, max_new_tokens = max_new_toks,output_hidden_states=False, output_attentions=False, output_logits=False,do_sample=False)

            generated_seqs = output
            genearated_seqs_unpadded = [seq[(seq != self.tokenizer.pad_token_id) * (seq != self.tokenizer.eos_token_id)] for seq in generated_seqs]
            if 'gemma-2' in self.tokenizer.name_or_path:
                # genearated_seqs_unpadded = [seq[0:-1] if seq[-1] == self.tokenizer.encode("<end_of_turn>")[0] else seq for seq in genearated_seqs_unpadded]
                # makeshift gemma cleanup
                e_token = self.tokenizer.encode("<end_of_turn>")[0]
                #genearated_seqs_unpadded = [seq[:len(seq) - len(seq) // 5 + torch.argmax(seq[-len(seq) // 5:] == e_token) + 1] if torch.any(seq[-len(seq) // 5:] == e_token) else seq for seq in genearated_seqs_unpadded]
                genearated_seqs_unpadded = [
                    seq[
                    :len(seq) - len(seq) // 5 + (seq[-len(seq) // 5:] == e_token).nonzero(as_tuple=True)[0][0] + 0]
                    if torch.any(seq[-len(seq) // 5:] == e_token) else seq
                    for seq in genearated_seqs_unpadded
                ]

                genearated_seqs_unpadded = [
                    torch.cat((seq[:-len(seq) // 5], seq[-len(seq) // 5:][seq[-len(seq) // 5: ] != e_token]))
                    for seq in genearated_seqs_unpadded
                ]
            target_slice_starts = [len(seq) for seq in genearated_seqs_unpadded]
            #focused_target_slice_starts = [len(seq) + offset for (seq, offset) in zip(genearated_seqs_unpadded, batched_focus_offset)]
            gen_target_ids = [torch.cat([gen,target.to(model.device)],dim=-1) for (gen,target) in zip(genearated_seqs_unpadded,batched_target)]
            original_lengths = [len(item) for item in gen_target_ids]
            # batched_target_gen_pa        dded = pad_sequence([item for item in gen_target_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id).to(model.device)
            # attention_mask_gen_target = (batched_target_gen_padded != self.tokenizer.pad_token_id).to(model.device)
            # Apply left-padding for generated sequences + target
            gen_target_ids_reversed = [torch.flip(item, dims=[0]) for item in gen_target_ids]
            batched_target_gen_padded = pad_sequence(gen_target_ids_reversed, batch_first=True, padding_value=self.tokenizer.pad_token_id).to(model.device)
            batched_target_gen_padded = torch.flip(batched_target_gen_padded, dims=[1])  # Flip them back

            padded_lengths = batched_target_gen_padded.size(1)

            # Calculate how many tokens were padded for each sample
            padding_counts = [padded_lengths - orig_len for orig_len in original_lengths]

            #target_slice_starts = [len(seq) for seq in batched_target_gen_padded]

            # Update attention masks accordingly
            attention_mask_gen_target = (batched_target_gen_padded != self.tokenizer.pad_token_id).to(model.device)

            del batched_ids_padded, attention_mask_ids_padded, generated_seqs, genearated_seqs_unpadded, output, gen_target_ids ; gc.collect()
            torch.cuda.empty_cache()

            with torch.no_grad():
                outputs_final = model(
                    input_ids=batched_target_gen_padded,
                    attention_mask=attention_mask_gen_target,
                    return_dict=False,
                    # past_key_values=past_key_values,
                    # use_cache=True,
                )
                
            

            logits_all.append(outputs_final[0])
            target_starts.extend(target_slice_starts)
            padding_counts_list.extend(padding_counts)

            del batched_target_gen_padded, attention_mask_gen_target, outputs_final; gc.collect()
            torch.cuda.empty_cache()

        print("Time taken to get recomputed solution logits: ", time.time() - stpwatch_start)
        del sliced_input_ids_list, temp, focused_targets_list ; gc.collect
        torch.cuda.empty_cache()
        return logits_all, target_starts, targets_list, _prompts_obj_list, padding_counts_list # we just need the logits, position of target, and actual target_list for calculating loss.

    def target_loss(self, logits, ids):
        return torch.cat(
            [
                prompt.target_loss(logit, id).mean(dim=1).unsqueeze(1)
                for prompt, logit, id in zip(self._prompts, logits, ids)
            ],
            dim=1
        ).mean(dim=1)

    def selection_loss(self, logits, target_slice_starts, target_ids, prompts_obj_list, padding_counts):


        loss1, loss2, loss3 = [], [], []
        index = 0

        for _logits in logits:
            batch_size = _logits.size(0)
            for i in range(batch_size):
                # prompt = self._prompts[index]
                prompt = prompts_obj_list[index]
                target_slice_start = target_slice_starts[index]
                target_id = target_ids[index]

                l1, l2, l3 = prompt.selection_loss(_logits[i][padding_counts[index]:], target_slice_start, target_id)
                #l1, l2, l3 = l1.mean(dim=1).unsqueeze(1), l2, l3.mean(dim=1).unsqueeze(1)
                l1, l2, l3 = l1.mean(dim=0), l2, l3.mean(dim=1).unsqueeze(1)

                loss1.append(l1)
                loss2.append(l2)
                loss3.append(l3)

                index += 1

        #loss1, loss2, loss3 = torch.cat(loss1, dim=1).mean(dim=1), torch.cat(loss2, dim=0), torch.cat(loss3, dim=1).mean(dim=1)
        loss1, loss2, loss3 = torch.cat(loss1, dim=0), torch.cat(loss2, dim=0), torch.cat(loss3,dim=1).mean(dim=1)

        return loss1, loss2, loss3


    def control_loss(self, logits, ids):
        return torch.cat(
            [
                prompt.control_loss(logit, id).mean(dim=1).unsqueeze(1)
                for prompt, logit, id in zip(self._prompts, logits, ids)
            ],
            dim=1
        ).mean(dim=1)

    def sample_control(self, *args, **kwargs):

        raise NotImplementedError("Sampling control tokens not yet implemented")

    def __len__(self):
        return len(self._prompts)

    def __getitem__(self, i):
        return self._prompts[i]

    def __iter__(self):
        return iter(self._prompts)

    @property
    def control_str(self):
        return self._prompts[0].control_str

    @property
    def control_toks(self):
        return self._prompts[0].control_toks

    @control_str.setter
    def control_str(self, control):
        for prompt in self._prompts:
            prompt.control_str = control

    @control_toks.setter
    def control_toks(self, control_toks):
        for prompt in self._prompts:
            prompt.control_toks = control_toks

    @property
    def disallowed_toks(self):
        return self._nonascii_toks


class MultiPrompter(object):
    """A class used to manage multiple prompt-based attacks."""

    def __init__(self,
                 goals,
                 targets,
                 workers,
                 control_init="! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !",
                 test_prefixes=["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"],
                 logfile=None,
                 managers=None,
                 test_goals=[],
                 test_targets=[],
                 test_workers=[],
                 train_final_targets=[],
                 test_final_targets=[],
                 *args, **kwargs
                 ):
        """
        Initializes the MultiPromptAttack object with the provided parameters.

        Parameters
        ----------
        goals : list of str
            The list of intended goals of the attack
        targets : list of str
            The list of targets of the attack
        workers : list of Worker objects
            The list of workers used in the attack
        control_init : str, optional
            A string used to control the attack (default is "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !")
        test_prefixes : list, optional
            A list of prefixes to test the attack (default is ["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"])
        logfile : str, optional
            A file to which logs will be written
        managers : dict, optional
            A dictionary of manager objects, required to create the prompts.
        test_goals : list of str, optional
            The list of test goals of the attack
        test_targets : list of str, optional
            The list of test targets of the attack
        test_workers : list of Worker objects, optional
            The list of test workers used in the attack
        """

        self.goals = goals
        self.targets = targets
        self.workers = workers
        self.test_goals = test_goals
        self.test_targets = test_targets
        self.test_workers = test_workers
        self.test_prefixes = test_prefixes
        self.models = [worker.model for worker in workers]
        self.logfile = logfile
        self.prompts = [
            managers['PM'](
                goals,
                targets,
                worker.tokenizer,
                worker.conv_template,
                control_init,
                test_prefixes,
                managers,
                train_final_targets
            )
            for worker in workers
        ]


        self.managers = managers
        self.train_final_targets = train_final_targets
        self.test_final_targets = test_final_targets

    @property
    def control_str(self):
        return self.prompts[0].control_str

    @control_str.setter
    def control_str(self, control):
        for prompts in self.prompts:
            prompts.control_str = control

        # for prompts in self.test_prompts:
        #     prompts.control_str = control

    @property
    def control_toks(self):
        return [prompts.control_toks for prompts in self.prompts]

    @control_toks.setter
    def control_toks(self, control):
        if len(control) != len(self.prompts):
            raise ValueError("Must provide control tokens for each tokenizer")
        for i in range(len(control)):
            self.prompts[i].control_toks = control[i]
            #self.test_prompts[i].control_toks = control[i]

    def update_solution(self):
        for prompt, worker in zip(self.prompts, self.test_workers):
            prompt.update_solution(worker.model)

    def get_filtered_cands(self, worker_index, control_cand, filter_cand=True, curr_control=None):
        cands, count = [], 0
        worker = self.workers[worker_index]
        for i in range(control_cand.shape[0]):
            decoded_str = worker.tokenizer.decode(control_cand[i], skip_special_tokens=True)
            if filter_cand:
                if decoded_str != curr_control and len(
                        worker.tokenizer(decoded_str, add_special_tokens=False).input_ids) == len(control_cand[i]):
                    cands.append(decoded_str)
                else:
                    count += 1
            else:
                cands.append(decoded_str)

        if filter_cand:
            cands = cands + [cands[-1]] * (len(control_cand) - len(cands))
            # print(f"Warning: {round(count / len(control_cand), 2)} control candidates were not valid")
        return cands

    def step(self, *args, **kwargs):

        raise NotImplementedError("Attack step function not yet implemented")

    def run(self,
            n_steps=100,
            batch_size=1024,
            topk=256,
            temp=1,
            topq=5,
            allow_non_ascii=False,
            target_weight=None,
            control_weight=None,
            anneal=True,
            anneal_from=0,
            prev_loss=np.infty,
            stop_on_success=True,
            test_steps=200,
            log_first=False,
            filter_cand=True,
            verbose=True,
            early_stopping=True,
            loss_threshold=0.12,
            early_stopping_steps=150,

            ):

        def P(e, e_prime, k):
            T = max(1 - float(k + 1) / (n_steps + anneal_from), 1.e-7)
            return True if e_prime < e else math.exp(-(e_prime - e) / T) >= random.random()

        best_step = 0

        if target_weight is None:
            target_weight_fn = lambda _: 1
        elif isinstance(target_weight, (int, float)):
            target_weight_fn = lambda i: target_weight
        if control_weight is None:
            control_weight_fn = lambda _: 0.1
        elif isinstance(control_weight, (int, float)):
            control_weight_fn = lambda i: control_weight

        steps = 0
        loss = best_loss = 1e6
        best_control = self.control_str
        top_controls = []
        runtime = 0.

        if self.logfile is not None and log_first:
            model_tests = self.test_all()
            self.log(anneal_from,
                     n_steps + anneal_from,
                     self.control_str,
                     loss,
                     runtime,
                     model_tests,
                     verbose=verbose)

        for i in range(n_steps):

            # if stop_on_success:
            #     model_tests_jb, model_tests_mb, _ = self.test(self.workers, self.prompts)
            #     if all(all(tests for tests in model_test) for model_test in model_tests_jb):
            #         break

            steps += 1
            start = time.time()
            torch.cuda.empty_cache()

            if SIMULATED_CANONICAL:
                self.update_solution()

            control, loss = self.step(
                batch_size=batch_size,
                topk=topk,
                temp=temp,
                topq=topq,
                allow_non_ascii=allow_non_ascii,
                target_weight=target_weight_fn(i),
                control_weight=control_weight_fn(i),
                filter_cand=filter_cand,
                verbose=verbose
            )
            runtime = time.time() - start
            keep_control = True if not anneal else P(prev_loss, loss, i + anneal_from)
            if keep_control:
                self.control_str = control
            else:
                self.control_str = control
                print('!!!!Rejecting new control originally, changed !!!!')

            # if SIMULATED_CANONICAL:
            #     self.update_solution()


            prev_loss = loss
            if loss < best_loss:
                best_loss = loss
                best_step = i
                best_control = control

            if len(top_controls) < 10 or loss < top_controls[-1][0]:
                if len(top_controls) == 10:
                    top_controls.pop()

                top_controls.append((loss, control))
                top_controls.sort(key=lambda x: x[0])

            print('Current Loss:', loss, 'Best Loss:', best_loss, 'Best Control:', best_control)

            if i%15 == 0:
                print("Step: ", i, "Candidates: ", top_controls)

            if loss < loss_threshold and early_stopping:
                print('Loss below loss_threshold. Moving to next objective.')
                break

            if i - best_step > early_stopping_steps and early_stopping:
                print(f'Loss plateaued for {early_stopping_steps} steps. Moving to next group optimization.')
                # self.control_str = best_control
                break

            if self.logfile is not None and (i + 1 + anneal_from) % test_steps == 0:
                last_control = self.control_str
                self.control_str = best_control

                model_tests = self.test_all()
                self.log(i + 1 + anneal_from, n_steps + anneal_from, self.control_str, best_loss, runtime, model_tests,
                         verbose=verbose)

                self.control_str = last_control

        # Added later

        return self.control_str, loss, steps

    def test(self, workers, prompts, include_loss=False):
        for j, worker in enumerate(workers):
            worker(prompts[j], "test", worker.model)
        model_tests = np.array([worker.results.get() for worker in workers])
        model_tests_jb = model_tests[..., 0].tolist()
        model_tests_mb = model_tests[..., 1].tolist()
        model_tests_loss = []
        if include_loss:
            for j, worker in enumerate(workers):
                worker(prompts[j], "test_loss", worker.model)
            model_tests_loss = [worker.results.get() for worker in workers]

        return model_tests_jb, model_tests_mb, model_tests_loss

    def test_all(self):
        all_workers = self.workers + self.test_workers
        all_prompts = [
            self.managers['PM'](
                self.goals + self.test_goals,
                self.targets + self.test_targets,
                worker.tokenizer,
                worker.conv_template,
                self.control_str,
                self.test_prefixes,
                self.managers
            )
            for worker in all_workers
        ]
        return self.test(all_workers, all_prompts, include_loss=True)

    def parse_results(self, results):
        x = len(self.workers)
        i = len(self.goals)
        id_id = results[:x, :i].sum()
        id_od = results[:x, i:].sum()
        od_id = results[x:, :i].sum()
        od_od = results[x:, i:].sum()
        return id_id, id_od, od_id, od_od

    def log(self, step_num, n_steps, control, loss, runtime, model_tests, verbose=True):

        prompt_tests_jb, prompt_tests_mb, model_tests_loss = list(map(np.array, model_tests))
        all_goal_strs = self.goals + self.test_goals
        all_workers = self.workers + self.test_workers
        tests = {
            all_goal_strs[i]:
                [
                    (all_workers[j].model.name_or_path, prompt_tests_jb[j][i], prompt_tests_mb[j][i],
                     model_tests_loss[j][i])
                    for j in range(len(all_workers))
                ]
            for i in range(len(all_goal_strs))
        }
        n_passed = self.parse_results(prompt_tests_jb)
        n_em = self.parse_results(prompt_tests_mb)
        n_loss = self.parse_results(model_tests_loss)
        total_tests = self.parse_results(np.ones(prompt_tests_jb.shape, dtype=int))
        n_loss = [l / t if t > 0 else 0 for l, t in zip(n_loss, total_tests)]

        tests['n_passed'] = n_passed
        tests['n_em'] = n_em
        tests['n_loss'] = n_loss
        tests['total'] = total_tests

        with open(self.logfile, 'r') as f:
            log = json.load(f)

        log['controls'].append(control)
        log['losses'].append(loss)
        log['runtimes'].append(runtime)
        log['tests'].append(tests)

        with open(self.logfile, 'w') as f:
            json.dump(log, f, indent=4, cls=NpEncoder)

        if verbose:
            output_str = ''
            for i, tag in enumerate(['id_id', 'id_od', 'od_id', 'od_od']):
                if total_tests[i] > 0:
                    output_str += f"({tag}) | Passed {n_passed[i]:>3}/{total_tests[i]:<3} | EM {n_em[i]:>3}/{total_tests[i]:<3} | Loss {n_loss[i]:.4f}\n"
            print((
                f"\n====================================================\n"
                f"Step {step_num:>4}/{n_steps:>4} ({runtime:.4} s)\n"
                f"{output_str}"
                f"control='{control}'\n"
                f"====================================================\n"
            ))


class ProgressiveMultiPrompter(object):
    """A class used to manage multiple progressive prompt-based attacks."""

    def __init__(self,
                 goals,
                 targets,
                 workers,
                 progressive_goals=True,
                 progressive_models=True,
                 control_init="! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !",
                 test_prefixes=["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"],
                 logfile=None,
                 managers=None,
                 test_goals=[],
                 test_targets=[],
                 test_workers=[],
                 train_final_target=[],
                 test_final_target=[],
                 *args, **kwargs
                 ):

        """
        Initializes the ProgressiveMultiPromptAttack object with the provided parameters.

        Parameters
        ----------
        goals : list of str
            The list of intended goals of the attack
        targets : list of str
            The list of targets of the attack
        workers : list of Worker objects
            The list of workers used in the attack
        progressive_goals : bool, optional
            If true, goals progress over time (default is True)
        progressive_models : bool, optional
            If true, models progress over time (default is True)
        control_init : str, optional
            A string used to control the attack (default is "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !")
        test_prefixes : list, optional
            A list of prefixes to test the attack (default is ["I'm sorry", "I apologize", "As an", "I'm just", "I'm an", "I cannot", "Hello!"])
        logfile : str, optional
            A file to which logs will be written
        managers : dict, optional
            A dictionary of manager objects, required to create the prompts.
        test_goals : list of str, optional
            The list of test goals of the attack
        test_targets : list of str, optional
            The list of test targets of the attack
        test_workers : list of Worker objects, optional
            The list of test workers used in the attack
        """

        self.goals = goals
        self.targets = targets
        self.workers = workers
        self.test_goals = test_goals
        self.test_targets = test_targets
        self.test_workers = test_workers
        self.progressive_goals = progressive_goals
        self.progressive_models = progressive_models
        self.control = control_init
        self.test_prefixes = test_prefixes
        self.logfile = logfile
        self.managers = managers
        self.mpa_kwargs = ProgressiveMultiPrompter.filter_mpa_kwargs(**kwargs)
        self.train_final_target = train_final_target
        self.test_final_target = test_final_target

        if logfile is not None:
            with open(logfile, 'w') as f:
                json.dump({
                    'params': {
                        'goals': goals,
                        'targets': targets,
                        'test_goals': test_goals,
                        'test_targets': test_targets,
                        'progressive_goals': progressive_goals,
                        'progressive_models': progressive_models,
                        'control_init': control_init,
                        'test_prefixes': test_prefixes,
                        'models': [
                            {
                                'model_path': worker.model.name_or_path,
                                'tokenizer_path': worker.tokenizer.name_or_path,
                                'conv_template': worker.conv_template.name
                            }
                            for worker in self.workers
                        ],
                        'test_models': [
                            {
                                'model_path': worker.model.name_or_path,
                                'tokenizer_path': worker.tokenizer.name_or_path,
                                'conv_template': worker.conv_template.name
                            }
                            for worker in self.test_workers
                        ]
                    },
                    'controls': [],
                    'losses': [],
                    'runtimes': [],
                    'tests': []
                }, f, indent=4
                )

    @staticmethod
    def filter_mpa_kwargs(**kwargs):
        mpa_kwargs = {}
        for key in kwargs.keys():
            if key.startswith('mpa_'):
                mpa_kwargs[key[4:]] = kwargs[key]
        return mpa_kwargs

    def run(self,
            n_steps: int = 1000,
            batch_size: int = 1024,
            topk: int = 256,
            temp: float = 1.,
            topq: int = 5,
            allow_non_ascii: bool = False,
            target_weight=None,
            control_weight=None,
            anneal: bool = True,
            test_steps: int = 50,
            incr_control: bool = True,
            stop_on_success: bool = True,
            verbose: bool = True,
            filter_cand: bool = False,
            group_size=100
            ):


        """
        Executes the progressive multi prompt attack.

        Parameters
        ----------
        n_steps : int, optional
            The number of steps to run the attack (default is 1000)
        batch_size : int, optional
            The size of batches to process at a time (default is 1024)
        topk : int, optional
            The number of top candidates to consider (default is 256)
        temp : float, optional
            The temperature for sampling (default is 1)
        topq : int, optional
            The number of top candidate
        allow_non_ascii : bool, optional
            Whether to allow non-ASCII characters (default is False)
        target_weight
            The weight assigned to the target
        control_weight
            The weight assigned to the control
        anneal : bool, optional
            Whether to anneal the temperature (default is True)
        test_steps : int, optional
            The number of steps between tests (default is 50)
        incr_control : bool, optional
            Whether to increase the control over time (default is True)
        stop_on_success : bool, optional
            Whether to stop the attack upon success (default is True)
        verbose : bool, optional
            Whether to print verbose output (default is True)
        filter_cand : bool, optional
            Whether to filter candidates whose lengths changed after re-tokenization (default is True)
        """

        if self.logfile is not None:
            with open(self.logfile, 'r') as f:
                log = json.load(f)

            log['params']['n_steps'] = n_steps
            log['params']['test_steps'] = test_steps
            log['params']['batch_size'] = batch_size
            log['params']['topk'] = topk
            log['params']['temp'] = temp
            log['params']['topq'] = topq
            log['params']['allow_non_ascii'] = allow_non_ascii
            log['params']['target_weight'] = target_weight
            log['params']['control_weight'] = control_weight
            log['params']['anneal'] = anneal
            log['params']['incr_control'] = incr_control
            log['params']['stop_on_success'] = stop_on_success

            with open(self.logfile, 'w') as f:
                json.dump(log, f, indent=4)

        num_goals = group_size if self.progressive_goals else len(self.goals)
        num_workers = group_size if self.progressive_models else len(self.workers)
        step = 0
        # stop_inner_on_success = self.progressive_goals
        stop_inner_on_success = False
        loss = np.infty
        early_stopping = True
        while step < n_steps:
            attack = self.managers['MPA'](
                self.goals[:num_goals],
                self.targets[:num_goals],
                self.workers[:num_workers],
                self.control,
                self.test_prefixes,
                self.logfile,
                self.managers,
                self.test_goals,
                self.test_targets,
                self.test_workers,
                self.train_final_target,
                self.test_final_target,
                **self.mpa_kwargs
            )
            if num_goals == len(self.goals) and num_workers == len(self.workers):
                stop_inner_on_success = False
                early_stopping = False
            control, loss, inner_steps = attack.run(
                n_steps=n_steps - step,
                batch_size=batch_size,
                topk=topk,
                temp=temp,
                topq=topq,
                allow_non_ascii=allow_non_ascii,
                target_weight=target_weight,
                control_weight=control_weight,
                anneal=anneal,
                anneal_from=step,
                prev_loss=loss,
                stop_on_success=stop_inner_on_success,
                test_steps=test_steps,
                filter_cand=filter_cand,
                early_stopping=early_stopping,
                verbose=verbose
            )

            step += inner_steps
            self.control = control

            if num_goals < len(self.goals):
                num_goals += group_size
                num_goals = min(num_goals, len(self.goals))
                loss = np.infty
            elif num_goals == len(self.goals):
                if num_workers < len(self.workers):
                    num_workers += group_size
                    num_workers = min(num_workers, len(self.workers))
                    loss = np.infty
                elif num_workers == len(self.workers) and stop_on_success:
                    model_tests = attack.test_all()
                    attack.log(step, n_steps, self.control, loss, 0., model_tests, verbose=verbose)
                    break
                else:
                    if isinstance(control_weight, (int, float)) and incr_control:
                        if control_weight <= 0.5:
                            control_weight += 0.01
                            loss = np.infty
                            if verbose:
                                print(f"Control weight increased to {control_weight:.5}")
                        else:
                            stop_inner_on_success = False

        return self.control, step


class ModelWorker(object):

    def __init__(self, model_path, model_kwargs, tokenizer, conv_template, device):
        if type(model_path) == type("sample string"):
            # os.environ["CUDA_VISIBLE_DEVICES"] = device
            # self.model = AutoModelForCausalLM.from_pretrained(
            #     model_path,
            #     torch_dtype=torch.float16,
            #     trust_remote_code=True,
            #     **model_kwargs
            # ).to(device).eval()

            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype='auto',
                trust_remote_code=True,
                cache_dir='/scratch2/share/model_files/huggingface',
                # device_map = 'auto',
                **model_kwargs
            ).to(device).eval()

            # devices = ast.literal_eval(device)
            # self.model = nn.DataParallel(self.model, device_ids=devices)
            # .cuda(device=devices[0])


        else:
            self.model = model_path

        self.tokenizer = tokenizer
        self.conv_template = conv_template
        self.tasks = mp.JoinableQueue()
        self.results = mp.JoinableQueue()
        self.process = None
        self.device_ = device

    @staticmethod
    def run(model, tasks, results):
        # torch.cuda.init()
        # os.environ["CUDA_VISIBLE_DEVICES"] = device_
        # torch.cuda.init()
        # model = AutoModelForCausalLM.from_pretrained(model.name_or_path, torch_dtype=torch.float16,
        #         trust_remote_code=True, device_map = 'auto').eval()

        # uncomment
        # model = dill.loads(model)
        # tasks = (tasks)
        # results = (results)

        while True:
            task = tasks.get()
            if task is None:
                break
            ob, fn, args, kwargs = task
            # args = dill.loads(args)
            # uncomment when want to use multiple gpus
            # args = list(args)
            # args[0] = model
            # args = tuple(args)

            args = (model,) + tuple(args[1:])

            if fn == "grad":
                with torch.enable_grad():
                    model.train()
                    results.put(ob.grad(*args, **kwargs))
            else:
                with torch.no_grad():
                    if fn == "logits":
                        results.put(ob.logits(*args, **kwargs))
                    elif fn == "logits_batched_gen":
                        results.put(ob.logits_batched_gen(*args, **kwargs))
                    elif fn == "contrast_logits":
                        results.put(ob.contrast_logits(*args, **kwargs))
                    elif fn == "test":
                        results.put(ob.test(*args, **kwargs))
                    elif fn == "test_loss":
                        results.put(ob.test_loss(*args, **kwargs))
                    else:
                        results.put(fn(*args, **kwargs))
            tasks.task_done()

    def start(self):
        # self.process = mp.Process(
        #     target=ModelWorker.run,
        #     args=(self.model, self.tasks, self.results),
        #     kwargs={'device_': self.device_}
        # )
        # self.process = mp.Process(
        #     target=ModelWorker.run,
        #     args=(self.model, self.tasks, self.results)
        # )

        self.process = mp.Process(
            target=ModelWorker.run,
            #args=(dill.dumps(self.model), (self.tasks), (self.results))  # uncomment and comment the next one
            args = ((self.model), (self.tasks), (self.results))
        )

        self.process.start()
        print(f"Started worker {self.process.pid} for model {self.model.name_or_path}")
        return self

    def stop(self):
        self.tasks.put(None)
        if self.process is not None:
            self.process.join()
        torch.cuda.empty_cache()
        return self

    def __call__(self, ob, fn, *args, **kwargs):
        # self.tasks.put((deepcopy(ob), fn, dill.dumps(args), kwargs))
        # print(args)
        # print(type(args))
        # args[0] = args[0].name_or_path

        # uncomment when want to use multiple gpus

        # args = list(args)
        # args[0] = args[0].name_or_path
        # args = tuple(args)

        args = (args[0].name_or_path,) + tuple(args[1:])

        self.tasks.put((deepcopy(ob), fn, (args), kwargs))
        return self


def get_workers(params, eval=False):
    tokenizers = []
    for i in range(len(params.tokenizer_paths)):
        tokenizer = AutoTokenizer.from_pretrained(
            params.tokenizer_paths[i],
            trust_remote_code=True,
            cache_dir='/scratch2/share/model_files/huggingface',
            **params.tokenizer_kwargs[i]
        )
        if 'oasst-sft-6-llama-30b' in params.tokenizer_paths[i]:
            tokenizer.bos_token_id = 1
            tokenizer.unk_token_id = 0
        if 'guanaco' in params.tokenizer_paths[i]:
            tokenizer.eos_token_id = 2
            tokenizer.unk_token_id = 0
        if 'llama-2' in params.tokenizer_paths[i] or 'Llama-2' in params.tokenizer_paths[i]:
            tokenizer.pad_token = tokenizer.unk_token
            tokenizer.padding_side = 'left'
        if 'falcon' in params.tokenizer_paths[i]:
            tokenizer.padding_side = 'left'
        #if not tokenizer.pad_token:
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizers.append(tokenizer)

    print(f"Loaded {len(tokenizers)} tokenizers")

    # raw_conv_templates = [
    #     get_conversation_template(template)
    #     for template in params.conversation_templates
    # ]

    raw_conv_templates = []

    for template in params.conversation_templates:
        t1 = get_conversation_template(template)
        if 'gemma-2' in template:
            t1.name = 'gemma-2'
        elif 'gemma' in template:
            t1.name = 'gemma'
        elif 'llama-3' in template:
            t1.name = 'llama-3'
        raw_conv_templates.append(t1)

    conv_templates = []
    for conv in raw_conv_templates:

        if conv.name == 'zero_shot':
            conv.roles = tuple(['### ' + r for r in conv.roles])
            conv.sep = '\n'
        elif conv.name == 'llama-2':
            conv.system = "<s>[INST] "  # forcing to use no system instruction
            conv.sep2 = conv.sep2.strip()
        elif conv.name == 'llama-3':
            conv.system = " "  # not used in the system

        elif conv.name == 'gemma':
            # conv.system = "<bos><start_of_turn>"
            conv.system = "<bos>"
            conv.roles = ('user\n', 'model\n')
            # Handle rest manually inside implementation to avoid any potential issues
        conv_templates.append(conv)

    print(f"Loaded {len(conv_templates)} conversation templates")
    workers = [
        ModelWorker(
            params.model_paths[i],
            params.model_kwargs[i],
            tokenizers[i],
            conv_templates[i],
            params.devices[i]
        )
        for i in range(len(params.model_paths))
    ]
    print("Initialized Workers...")
    if not eval:
        for worker in workers:
            worker.start()

    num_train_models = getattr(params, 'num_train_models', len(workers))
    print('Loaded {} train models'.format(num_train_models))
    print('Loaded {} test models'.format(len(workers) - num_train_models))

    return workers[:num_train_models], workers[num_train_models:]

def remove_parentheses_if_single_char(input_string):
    if input_string.startswith('(') and input_string.endswith(')') and len(input_string) == 3:
        return input_string[1:-1]
    return input_string


def get_goals_and_targets(params, addition=""" Put **only** the final number around <ans>...</ans> tag at the end"""):
    addition = ". Use"
    addition = ". Use"
    addition2 = " The final answer to the original question is "
    addition2 = "Therefore, the final numerical answer is ($ result, where result is numerical) $ "
    addition2 = "Therefore, the final answer ($ Yes or $ No) is  $ "
    addition2 = "Therefore, the final answer ($ True or $ False) is  $ " # Type 4 in our list
    addition2 = "Therefore, the final answer ($ valid or $ invalid) is  $ "  # Type 5 in our list
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E) or $ (F)) is  $ "  # Type 1 in our list: date_understanding
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E) or $ (F) or $ (G) or $ (H) or $ (I) or $ (J)) is  $ "  # Type 1 in our list: geometric_shapes
    addition2 = "Therefore, the final answer ($ (A) or $ (B)) is  $ " # Type 1 in our list: hyperbaton
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E)) is  $ "  # Type 1 in our list: logical_deduction_five_objects
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E) or $ (F) or $ (G)) is  $ "  # Type 1 in our list: logical_deduction_seven_objects
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E)) is  $ " # Type 1 in our list: movie_recommendation
    addition2 = "Therefore, the final answer ($ (A) or $ (B) or $ (C) or $ (D) or $ (E)) is  $ "  # Type 1 in our list: penguins_in_a_table
    addition2 = "Therefore, the final answer ($ A or $ B or $ C) is  $ "  # Type 1 in our list: disambiguation_qa

    addition2 = "Therefore, the final answer ($ A or $ B or $ C or $ D or $ E or $ F) is  $ "  # Type 1 in our list: salient_translation_error_detection
    addition2 = "Therefore, the final answer ($ A or $ B or $ C or $ D) is  $ "  # Type 1 in our list: temporal_sequences
    addition2 = "Therefore, the final answer ($ A or $ B or $ C) is  $ "  # Type 1 in our list: snarks
    addition2 = "Therefore, the final answer ($ A or $ B or $ C or $ D or ... or $ R or $ S) is  $ "  # Type 1 in our list: reasoning_about_colored_objects


    #addition2 = " The final answer (Yes or No) to the question of truthfulness is: "
    addition3 = ""
    has_final_targets = False
    train_goals = getattr(params, 'goals', [])
    train_targets = getattr(params, 'targets', [])

    test_goals = getattr(params, 'test_goals', [])
    test_targets = getattr(params, 'test_targets', [])

    # additionally added to facilitate final loss
    train_final_targets = getattr(params, 'final_target', [])
    test_final_targets = getattr(params, 'final_target', [])

    addition2 = params.extractor_text

    offset = getattr(params, 'data_offset', 0)

    if params.train_data:
        if params.train_data.endswith('.tsv'):
            train_data = pd.read_csv(params.train_data, sep='\t', dtype=str)
        else:
            train_data = pd.read_csv(params.train_data, dtype=str)
        #train_targets = train_data['target'].astype(str).tolist()[offset:offset + params.n_train_data]
        train_targets = train_data['final_target'].astype(str).tolist()[offset:offset + params.n_train_data]
        if len(addition2) > 0:
            train_targets = [addition2 + remove_parentheses_if_single_char(target) for target in train_targets]

        if 'goal' in train_data.columns:
            train_goals = train_data['goal'].astype(str).tolist()[offset:offset + params.n_train_data]
            if len(addition) > 0:
                train_goals = [goal + addition for goal in train_goals]

        else:
            train_goals = [""] * len(train_targets)

        # Most datasets won't have it. Just the ones we are curating for this feature
        if 'final_target' in train_data.columns:
            train_final_targets = train_data['final_target'].astype(str).tolist()[offset:offset + params.n_train_data]
            if len(addition3) >= 0 and "llama-3" in params.conversation_templates:
                train_final_targets = [addition3 + remove_parentheses_if_single_char(target) for target in train_final_targets]
            elif "llama-2" in params.conversation_templates or "gemma-2" in params.conversation_templates:
                train_final_targets = [remove_parentheses_if_single_char(target) for target in train_final_targets]

            has_final_targets = True
        else:
            train_final_targets = [""] * len(train_targets)

        if params.test_data and params.n_test_data > 0:
            if params.test_data.endswith('.tsv'):
                test_data = pd.read_csv(params.test_data, sep='\t', dtype=str)
            else:
                test_data = pd.read_csv(params.test_data)
            #test_targets = test_data['target'].astype(str).tolist()[offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
            test_targets = test_data['final_target'].astype(str).tolist()[
                           offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
            if 'goal' in test_data.columns:
                test_goals = test_data['goal'].astype(str).tolist()[offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
                if len(addition) > 0:
                    test_goals = [goal + addition for goal in test_goals]
                if len(addition2) > 0:
                    test_targets = [addition2 + remove_parentheses_if_single_char(target) for target in test_targets]
            else:
                test_goals = [""] * len(test_targets)

            # Again, Most datasets won't have it. Just the ones we are curating for this feature
            if 'final_target' in test_data.columns:
                test_final_targets = test_data['final_target'].astype(str).tolist()[offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
                if len(addition3) >= 0 and "llama-3" in params.conversation_templates:
                    test_final_targets = [addition3 + remove_parentheses_if_single_char(target) for target in test_final_targets]
                elif "llama-2" in params.conversation_templates or "gemma-2" in params.conversation_templates:
                    train_final_targets = [remove_parentheses_if_single_char(target) for target in train_final_targets]

            else:
                test_final_targets = [""] * len(train_targets)

        elif params.n_test_data > 0:

            test_targets = train_data['target'].astype(str).tolist()[
                           offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
            if 'goal' in train_data.columns:
                test_goals = train_data['goal'].astype(str).tolist()[
                             offset + params.n_train_data:offset + params.n_train_data + params.n_test_data]
                if len(addition) > 0:
                    test_goals = [goal + addition for goal in test_goals]
            else:
                test_goals = [""] * len(test_targets)

    assert len(train_goals) == len(train_targets)
    assert len(test_goals) == len(test_targets)
    print('Loaded {} train goals'.format(len(train_goals)))
    print('Loaded {} test goals'.format(len(test_goals)))

    return train_goals, train_targets, test_goals, test_targets, train_final_targets, test_final_targets